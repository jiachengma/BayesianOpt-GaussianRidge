{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obse_pred_plot(X_mea, X_pred):\n",
    "    fig, ax = plt.subplots()\n",
    "    _x = np.linspace( np.min(X_mea), np.max(X_mea), 100 )\n",
    "    ax.plot(_x, _x, 'r--', label='$x=y$')\n",
    "    ax.plot(X_mea, X_pred, 'o')\n",
    "    ax.set_title('Predictions vs Measurements', fontsize=15)\n",
    "    ax.set_xlabel('Measured', fontsize=15)\n",
    "    ax.set_ylabel('Predicted', fontsize=15)\n",
    "    ax.tick_params( labelsize=15 )\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from pymanopt.manifolds import Stiefel\n",
    "from pymanopt.optimizers import ConjugateGradient, SteepestDescent, TrustRegions\n",
    "from pymanopt import Problem\n",
    "import pymanopt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class grf:\n",
    "    def __init__(self, X, Y, m_ridge, n_restart=20, tol=1e-2, test_size=0.5):\n",
    "        \"\"\"\n",
    "        X          -- input data\n",
    "        Y          -- output data\n",
    "        m_ridge    -- ridge function input dimension\n",
    "        n_restart  -- number of times to restart fitting and pick the model with the lowest objective function value\n",
    "        tol        -- error tolerance of cost to stop iteration\n",
    "        test_size  -- size to split data to train and test sets [0, 1]\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=20)\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.n_restart = n_restart\n",
    "        self.tol = tol\n",
    "        dim = X.shape[1] # original dimension\n",
    "        self.manifold = Stiefel(dim, m_ridge)\n",
    "        # initialize projection matrix M\n",
    "        V = np.random.randn(dim, m_ridge)\n",
    "        q = np.linalg.qr(V)[0]\n",
    "        self.M = q.copy()\n",
    "        # initialize covariance kernel\n",
    "        self.kernel = RBF\n",
    "    \n",
    "\n",
    "#     def create_cost_and_derivative(self):\n",
    "\n",
    "#         @pymanopt.function.numpy(self.manifold)\n",
    "#         def cost(M):\n",
    "#             U_train = self.X_train @ M\n",
    "#             U_test = self.X_test @ M\n",
    "\n",
    "#             G = self.kernel(U_train)\n",
    "#             b = np.linalg.solve(G, self.y_train)\n",
    "\n",
    "#             K_test = self.kernel(U_test, U_train)\n",
    "#             g_test = K_test @ b\n",
    "\n",
    "#             r = 0.5 * np.linalg.norm(self.y_test - g_test)**2 / self.y_test.shape[0]\n",
    "#             return r\n",
    "\n",
    "#         @pymanopt.function.numpy(self.manifold)\n",
    "#         def dcost(M):\n",
    "#             ell = self.kernel.get_params()['k1__k2__length_scale']\n",
    "#             U_train = self.X_train @ M\n",
    "#             U_test = self.X_test @ M\n",
    "#             N_test = self.y_test.shape[0]\n",
    "\n",
    "#             G = self.kernel(U_train)\n",
    "#             b = np.linalg.solve(G, self.y_train)\n",
    "#             K_test = self.kernel(U_test, U_train)\n",
    "#             g_test = K_test @ b\n",
    "\n",
    "#             inv_P = np.diag(1.0/ell**2)\n",
    "#             dr = np.zeros(M.shape)\n",
    "#             for i in range(N_test):\n",
    "#                 U_tilde = U_test[i] - U_train\n",
    "#                 dgdu = inv_P @ U_tilde.T @ (K_test[i,:] * b)\n",
    "#                 dy = np.outer(dgdu, self.X_test[i,:]).T\n",
    "#                 assert(dy.shape == M.shape)\n",
    "#                 dr += (self.y_test[i] - g_test[i]) * (dy - M @ dy.T @ M)\n",
    "#             return dr / N_test\n",
    "        \n",
    "#         return cost, dcost\n",
    "\n",
    "    def create_cost(self):\n",
    "        @pymanopt.function.autograd(self.manifold)\n",
    "        def cost(M):\n",
    "            U_train = self.X_train @ M\n",
    "            U_test = self.X_test @ M\n",
    "\n",
    "            N_train, m = self.X_train.shape\n",
    "\n",
    "            lengthscales = self.kernel.get_params()['k1__k2__length_scale'] # rbf lengthscale\n",
    "            sigma2_f = self.kernel.get_params()['k1__k1__constant_value'] # rbf variance\n",
    "            sigma2_n = self.kernel.get_params()['k2__noise_level'] # noise variance\n",
    "            L_inv = np.diag(1. / lengthscales)\n",
    "            dim = lengthscales.shape[0] # dimension of ridge function space\n",
    "\n",
    "            U_train_tilde = U_train @ L_inv \n",
    "            # covariance on training data\n",
    "            G = sigma2_f * np.exp(-0.5*(np.sum(U_train_tilde**2,1).reshape(-1,1) + np.sum(U_train_tilde**2,1) - \n",
    "                                       2 * np.dot(U_train_tilde, U_train_tilde.T)))\n",
    "\n",
    "            G = G + sigma2_n * np.eye(N_train)\n",
    "            b = np.linalg.solve(G, self.y_train)\n",
    "\n",
    "            N_test = self.X_test.shape[0]\n",
    "            U_test_tilde = U_test @ L_inv\n",
    "            # covariance of testing and training data K(U_test, U_train)\n",
    "            K_test = sigma2_f * np.exp(-0.5*(np.sum(U_test_tilde**2,1).reshape(-1,1) + np.sum(U_train_tilde**2,1) - \n",
    "                                       2 * np.dot(U_test_tilde, U_train_tilde.T)))\n",
    "            g_test = K_test @ b\n",
    "            r = 0.5 * np.linalg.norm(self.y_test - g_test)**2 / N_test\n",
    "            return r\n",
    "        return cost\n",
    "\n",
    "    def pred(self, X_test_pred, return_var=False):\n",
    "        \"\"\" \n",
    "        X_test_pred: test points to evaluate ridge function outputs\n",
    "        \n",
    "        Return:\n",
    "        g_test: predictions of posterior mean using ridge function\n",
    "        var_test: posterior variance at test points\n",
    "        \"\"\"\n",
    "        U_train = self.X_train @ self.M\n",
    "        U_test = X_test_pred @ self.M\n",
    "\n",
    "        G = self.kernel(U_train)\n",
    "        b = np.linalg.solve(G, self.y_train)\n",
    "\n",
    "        sigma2_n = self.kernel.get_params()['k2__noise_level'] # noise variance\n",
    "        K_test_train = self.kernel(U_test, U_train) # covariance of testing and training data\n",
    "        g_test = K_test_train @ b # predicted posterior mean\n",
    "        \n",
    "        if not return_var:\n",
    "            return g_test\n",
    "        else:\n",
    "            K_test = self.kernel(U_test)\n",
    "            b_test = np.linalg.solve(G, self.kernel(U_train, U_test))\n",
    "            cov_test = K_test - K_test_train @ b_test # posterior covariance\n",
    "            var_test = np.diag(cov_test)\n",
    "            return g_test, var_test\n",
    "            \n",
    "            \n",
    "            \n",
    "    def set_XY(self, X_new, Y_new):\n",
    "        \"\"\"\n",
    "        Update GPR model dataset\n",
    "        \"\"\"\n",
    "        self.X_train = np.vstack((self.X_train, X_new))\n",
    "        self.y_train = np.hstack((self.y_train, Y_new))\n",
    "        \n",
    "    @staticmethod\n",
    "    def BIC(gpr):\n",
    "        \"\"\"\n",
    "        Return BIC using log-likelihood\n",
    "        \"\"\"\n",
    "        return gpr.log_marginal_likelihood_value_ - 0.5 * (gpr.n_features_in_ + 2) * math.log(gpr.y_train_.shape[0])\n",
    "\n",
    "    def grf_fit(self):\n",
    "        last_r =1e10\n",
    "        err = np.inf\n",
    "        d, m = self.M.shape\n",
    "        n_iter = 0\n",
    "        \n",
    "        # re-initialize projection matrix M\n",
    "        V = np.random.randn(d, m)\n",
    "        q = np.linalg.qr(V)[0]\n",
    "        self.M = q.copy()\n",
    "        \n",
    "        while err > self.tol:\n",
    "            M_guess = self.M.copy()\n",
    "            n_iter += 1\n",
    "            U_train = self.X_train @ M_guess\n",
    "            # prior covariance\n",
    "            ker = 1.0 * RBF(length_scale=[1 for _ in range(m)], length_scale_bounds=(1e-7, 1e7)) \\\n",
    "            + WhiteKernel(noise_level=1e-4, noise_level_bounds=(1e-8, 1e2)) # noise_level: iid noise variance\n",
    "            gpr = GaussianProcessRegressor(kernel=ker, n_restarts_optimizer=20, alpha=1e-8, normalize_y=False) \n",
    "            # n_restars_optimizer: number of optimizations for hyper-parameters\n",
    "            # alpha: adding to diagonal of covariance matrix to prevent numerical issue during fitting\n",
    "            gpr.fit(U_train, self.y_train)\n",
    "\n",
    "            self.kernel = gpr.kernel_ # posterior kernel\n",
    "\n",
    "#             my_cost, my_dcost = self.create_cost_and_derivative()\n",
    "#             problem = Problem(manifold=self.manifold, cost=my_cost, euclidean_gradient=my_dcost)\n",
    "            my_cost = self.create_cost()\n",
    "            problem = Problem(manifold=self.manifold, cost=my_cost)\n",
    "#             optimizer = ConjugateGradient(verbosity=0)\n",
    "            optimizer = SteepestDescent(verbosity=0)\n",
    "#             optimizer = TrustRegions(verbosity=0)\n",
    "            M_new = optimizer.run(problem).point\n",
    "            self.M = M_new.copy()\n",
    "\n",
    "            r = my_cost(self.M)\n",
    "            err = np.abs(last_r - r) / last_r\n",
    "            last_r = r\n",
    "        bic = self.BIC(gpr)\n",
    "        return M_guess, gpr, r, bic, n_iter\n",
    "    \n",
    "    def __call__(self):\n",
    "        r_min = np.inf\n",
    "        for _ in range(self.n_restart):\n",
    "            M, gpr, r, bic, n_iter = self.grf_fit();\n",
    "            if r < r_min:\n",
    "                M_opt = M.copy()\n",
    "                gpr_opt = gpr\n",
    "                bic_opt = bic\n",
    "                n_final = n_iter\n",
    "                r_min = r\n",
    "        return M_opt, gpr_opt, r_min, bic_opt, n_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing linear ridge function\n",
    "Paper section 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training and testing data\n",
    "d = 10 # dimension of input\n",
    "m = 2 # ridge subspace dimension\n",
    "N = 100\n",
    "X = np.random.rand(N, d) * 2 - 1 # x in [-1,1]\n",
    "X_test = np.random.rand(50, d) * 2 - 1\n",
    "# training and testing data\n",
    "Ureal = np.random.randn(d, m)\n",
    "q = np.linalg.qr(Ureal)[0]\n",
    "Ureal = q.copy() # orthogonal\n",
    "U_data = X @ Ureal\n",
    "U_test = X_test @ Ureal\n",
    "Y = np.sum(U_data, axis=1)\n",
    "y_test = np.sum(U_test, axis=1)\n",
    "grf_test = grf(X, Y, 2, n_restart=1, tol=1e-2, test_size=0.3)\n",
    "results_grf = grf_test()\n",
    "obse_pred_plot(y_test, grf_test.pred(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grf_test.X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bic = results_grf[4]\n",
    "r = results_grf[2]\n",
    "print(f'cost={r}')\n",
    "print(f'BIC = {bic}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify sklearn predict method to calculate posterior mean \n",
    "M_final = results_grf[0]\n",
    "gpr_final = results_grf[1]\n",
    "obse_pred_plot(grf_test.pred(X_test), gpr_final.predict(X_test @ M_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify posterior variancey\n",
    "y_pred, std = gpr_final.predict(X_test @ M_final, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn gpr.predict gives same results as manually compute posterior mean, so gpr.predict can be used for mean and variance\n",
    "M_opt = results_grf[0]\n",
    "gpr_opt = results_grf[1]\n",
    "obse_pred_plot(grf_test.pred(X), gpr_opt.predict(X @ M_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ridge function\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "U_test_final = X_test @ M_opt # using optimized M\n",
    "ax.scatter(U_test_final[:,0], U_test_final[:,1], y_test, c=y_test)\n",
    "# ax.invert_yaxis()\n",
    "ax.set_xlabel('$m_1$')\n",
    "ax.set_ylabel('$m_2$')\n",
    "ax.set_zlabel('$f$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dymola.dymola_interface import DymolaInterface\n",
    "dymola = None\n",
    "dymola = DymolaInterface()\n",
    "dymola.openModel(path=\"C:\\Jiacheng Ma\\Modelica libraries\\DynamicVCC\\DynamicVCC\\package.mo\",changeDirectory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_HX(theta_in):\n",
    "    problem = \"DynamicVCC.Examples.Tests.Test_ShellTubeHX\"\n",
    "    startTime = 2000\n",
    "    stopTime = 3500\n",
    "    outputInterval = 10\n",
    "#     numberOfIntervals = 500\n",
    "    method = \"Dassl\"\n",
    "    tolerance = 0.0001\n",
    "    initialNames = ['u[{}]'.format(i) for i in range(1,len(theta_in)+1)]\n",
    "    initialValues = theta_in\n",
    "    dymola.experimentSetupOutput(events=False)\n",
    "    result, finalVar = dymola.simulateExtendedModel(problem=problem,\n",
    "                                          startTime=startTime, \n",
    "                                          stopTime=stopTime,\n",
    "                                          outputInterval=outputInterval,\n",
    "                                          method=method,\n",
    "                                          tolerance=tolerance,\n",
    "                                          initialNames=initialNames,\n",
    "                                          initialValues=initialValues)\n",
    "    if not result:\n",
    "        print(theta_in)\n",
    "        print(\"Simulation failed. Below is the translation log.\")\n",
    "        log = dymola.getLastErrorLog()\n",
    "        print(log)\n",
    "        exit(1)\n",
    "        return None, None\n",
    "    else:\n",
    "        Nrows = dymola.readTrajectorySize(\"dsres.mat\")\n",
    "        outputNames = ['y[{}]'.format(i) for i in range(1,4)] + ['y_mea[{}]'.format(i) for i in range(1,4)]\n",
    "        outputVar = dymola.readTrajectory(\"dsres.mat\", outputNames, Nrows)\n",
    "        pred = np.array(outputVar[:3])\n",
    "        Mea = np.array(outputVar[3:])\n",
    "        ner = np.linalg.norm(pred[:,10:] - Mea[:,10:], axis=1) / np.linalg.norm(Mea[:,10:], axis=1) # omit some initialization points\n",
    "        W = np.eye(ner.shape[0])\n",
    "        cost = np.dot(ner.T,W.dot(ner))\n",
    "        return cost, outputVar\n",
    "\n",
    "# Objective function to minimize\n",
    "def J_calib(u, lb, ub):\n",
    "    \"\"\"\n",
    "    u    -- Scaled HTC\n",
    "    lb   -- HTC lower bound\n",
    "    ub   -- HTC upper bound\n",
    "    \"\"\"\n",
    "    u_truescale = np.round(lb + u * (ub - lb),1)\n",
    "    cost, outputVar = L_HX(list(u_truescale))\n",
    "    if not cost:\n",
    "        exit(1)\n",
    "    else:\n",
    "        return -np.log(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dymola is not None:\n",
    "    dymola.close()\n",
    "    dymola = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dymola model\n",
    "theta_test = [5e4, 5e4, 5e4, 5e4, 131146]\n",
    "cost, outputVar = L_HX(theta_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Latin Hypercube designs to generate random samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some starting data\n",
    "np.random.seed(12345) # repeatable\n",
    "n_init = 200 # Number of data points\n",
    "lb = np.array([1e4,1e4,1e4,1e3,65573]) # Lower bounds of input space\n",
    "ub = np.array([1e6,1e6,1e6,1e6,196719]) # Upper bounds of input space\n",
    "\n",
    "from pyDOE import lhs\n",
    "\n",
    "# Generate scaled samples of the input space\n",
    "# X_normalize = np.random.rand(n_init, len(lb))\n",
    "X_normalize = lhs(len(lb), n_init, 'c')\n",
    "\n",
    "# Get corresponding results at function space\n",
    "Y = np.zeros(n_init)\n",
    "for i in range(n_init):\n",
    "    Y[i] = J_calib(X_normalize[i,:], lb, ub)\n",
    "    print(i+1, Y[i])\n",
    "X_normalize = X_normalize[~np.isnan(Y),:]\n",
    "Y = Y[~np.isnan(Y)]\n",
    "\n",
    "# Plot funciton values\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(Y,'kx',markersize=10, markeredgewidth=2)\n",
    "ax.set_xlabel('$n$')\n",
    "ax.set_ylabel('$J(u)$')\n",
    "\n",
    "# save data\n",
    "np.savetxt('YX_chillerCond.txt', np.hstack((Y[:,None], X_normalize)), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian ridge function for calibration parameter space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YX_data = np.loadtxt('YX_chillerCond.txt', delimiter=',')\n",
    "Y = YX_data[:,0]\n",
    "X_normalize = YX_data[:,1:]\n",
    "\n",
    "# normalize training data\n",
    "from sklearn import preprocessing\n",
    "scaler_y = preprocessing.StandardScaler().fit(Y[:,None])\n",
    "Y_scaled = scaler_y.transform(Y[:,None])\n",
    "\n",
    "scaler_X = preprocessing.StandardScaler().fit(X_normalize)\n",
    "X_scaled = scaler_X.transform(X_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grf_HX = grf(X_scaled, Y_scaled[:,0], 2, n_restart=30, tol=1e-2)\n",
    "results_HX = grf_HX()\n",
    "M_final, gpr_final = results_HX[0], results_HX[1]\n",
    "obse_pred_plot(Y, scaler_y.inverse_transform(grf_HX.pred(X_scaled)[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengthscales = gpr_final.kernel_.get_params()['k1__k2__length_scale'] # rbf lengthscale\n",
    "sigma2_f = gpr_final.kernel_.get_params()['k1__k1__constant_value'] # rbf variance\n",
    "sigma2_n = gpr_final.kernel_.get_params()['k2__noise_level'] # noise variance\n",
    "L_inv = np.diag(1. / lengthscales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_train = gpr_final.X_train_\n",
    "U_test = np.random.rand(20,2)\n",
    "U_train_tilde = U_train @ L_inv \n",
    "U_test_tilde = U_test @ L_inv\n",
    "# covariance on training data\n",
    "G = sigma2_f * np.exp(-0.5*(np.sum(U_test_tilde**2,1).reshape(-1,1) + np.sum(U_train_tilde**2,1) - \n",
    "                           2 * np.dot(U_test_tilde, U_train_tilde.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_est = gpr_final.kernel_(U_test, U_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance on training data\n",
    "y_train = scaler_y.inverse_transform(grf_HX.y_train[:,None])\n",
    "y_train_pred = scaler_y.inverse_transform(grf_HX.pred(grf_HX.X_train)[:,None])\n",
    "obse_pred_plot(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare gpr.predict() method and grf.pred() method\n",
    "y_train_predict = scaler_y.inverse_transform(gpr_final.predict(grf_HX.X_train @ M_final)[:,None])\n",
    "y_train_pred = scaler_y.inverse_transform(grf_HX.pred(grf_HX.X_train)[:,None])\n",
    "obse_pred_plot(y_train_predict, y_train_pred)\n",
    "# for using gpr.predict, input should be projected onto M_final, which is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance only on testing data\n",
    "X_test_scaled = grf_HX.X_test\n",
    "y_test_scaled = grf_HX.y_test\n",
    "obse_pred_plot(scaler_y.inverse_transform(y_test_scaled[:,None]), scaler_y.inverse_transform(grf_HX.pred(X_test_scaled)[:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'r={results_HX[2]}')\n",
    "print(f'BIC={results_HX[3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_final.optimizer=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_final.kernel_.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGOmaximize(f, gpr, X_design, alpha, f_params={}, alpha_params={}, max_it=15, optimize_it=100, plot=False):\n",
    "    \"\"\"Optimize a function using Bayesian global optimization\n",
    "    Arguments\n",
    "    f              -- The function to optimize\n",
    "    gpr            -- Gaussian process regression model to approximate the objective function\n",
    "    alpha          -- Information acquisition function\n",
    "    alpha_params   -- Extra parameters to the information acquisition function\n",
    "    X_design       -- The set of candidate points to evaluate the function for identifying the optimal point\n",
    "    max_it         -- The maximum number of iterations\n",
    "    plot           -- Whether or not to plot function evaluations v.s. iterations at max_it\n",
    "    optimize_it    -- Iterations after to optimize the hyper-parameters of GPR model\n",
    "    \"\"\"\n",
    "    af_all = [] # Store values of acquisition function \n",
    "    x_all = []\n",
    "    y_all = []\n",
    "    gpr.n_restarts_optimizer=0\n",
    "    for count in range(max_it):\n",
    "        # Using GPR model to get posterior mean and variance at given design points\n",
    "        m, sigma = gpr.predict(X_design, return_std=True) # posterior mean and standard deviation\n",
    "        # Evaluate information acquisition function\n",
    "        af_values = alpha(m, sigma, gpr.y_train_.max(), **alpha_params)\n",
    "        # Find index of the next point to evaluate\n",
    "        i = np.argmax(af_values)\n",
    "        # Evaluate the function and stack the new data point to observations\n",
    "        x_new = np.linalg.solve(M_final @ M_final.T, M_final @ U_design[i, :].reshape(-1,1))\n",
    "        y_new = f(np.clip(x_new.squeeze(),0.,1.), **f_params)\n",
    "        print(count+1, x_new, y_new)\n",
    "        if not y_new:\n",
    "            X_design = np.delete(X_design, i, axis=0)\n",
    "        else:\n",
    "            x_all.append(x_new)\n",
    "            y_all.append(y_new)\n",
    "            af_all.append(af_values[i])\n",
    "            u_new = M_final.T @ x_new\n",
    "            # Update GPR\n",
    "            gpr = grf_HX.set_XY(gpr, u_new.T, y_new[None])\n",
    "            \n",
    "            \n",
    "    if plot:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(y_all, '-*', markersize=10, markeredgewidth=2)\n",
    "        ax.set_xticks(range(1,max_it+1,2))\n",
    "        ax.set_xlabel('Iterations')\n",
    "        ax.set_ylabel('$f(x)$')\n",
    "            \n",
    "    return af_all, x_all, y_all\n",
    "\n",
    "# maximum upper interval\n",
    "def mui(m, sigma, ymax, psi=1.96):\n",
    "    return m + psi * sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_design = int(1e6)\n",
    "max_it=20 # number of iterations\n",
    "X_design_normalize = np.random.rand(n_design, len(lb))\n",
    "U_design = X_design_normalize @ M_final\n",
    "gpr_final.n_restarts_optimizer = 0\n",
    "af_all, x_all, y_all = BGOmaximize(J_calib, gpr_final, U_design, mui, f_params={'lb':lb,'ub':ub},max_it=max_it,\n",
    "                                   optimize_it=500, plot=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpr_final.y_train_.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, sigma = gpr_final.predict(U_design, return_std=True) # posterior mean and standard deviation\n",
    "# Evaluate information acquisition function\n",
    "af_values = mui(m, sigma, gpr_final.y_train_.max())\n",
    "# Find index of the next point to evaluate\n",
    "i = np.argmax(af_values)\n",
    "# Evaluate the function and stack the new data point to observations\n",
    "x_new = np.linalg.solve(M_final @ M_final.T, M_final @ U_design[i, :].reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
